{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from prepared_data.get_prepared_data import get_prepared_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from prepared_data.train_model import split_features_target_and_map_target, save_results, callbacks, columns_only_first_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent of object with nan value and orginals:  0.00, 85.915556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrzej/PycharmProjects/Physionet_Challenge_2021/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3251: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent of object with nan value and orginals:  0.00, 85.915556\n"
     ]
    }
   ],
   "source": [
    "train_org = pd.read_csv('data/train.csv')\n",
    "train,_,_ = get_prepared_data(train_org,map_target=False)\n",
    "# train = pd.read_csv('data/train_made.csv',index_col=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "['is_cup',\n 'home_team_history_is_play_home_1',\n 'away_team_history_is_play_home_1',\n 'home_team_history_is_cup_1',\n 'away_team_history_is_cup_1',\n 'home_team_history_coach_1',\n 'away_team_history_coach_1',\n 'home_team_history_target_1_-1.0',\n 'away_team_history_target_1_-1.0',\n 'home_team_history_target_1_0.0',\n 'away_team_history_target_1_0.0',\n 'home_team_history_target_1_1.0',\n 'away_team_history_target_1_1.0']"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_features, target = split_features_target_and_map_target(train)\n",
    "\n",
    "# normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "# normalizer.adapt(np.array(train_features))\n",
    "\n",
    "# normalizer.trainable_variables\n",
    "# normalizer(train_features)\n",
    "binary_col = ['is_cup','_team_history_is_play_home_1','_team_history_is_cup_1','_team_history_coach_1','_team_history_target_1_-1.0', '_team_history_target_1_0.0', '_team_history_target_1_1.0']\n",
    "\n",
    "binary_col_all = ['is_cup']\n",
    "binary_col_all.extend([ home_or_away +i  for i in  binary_col[1:] for home_or_away in ['home','away'] ])\n",
    "binary_col_all\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\n",
    "def build_and_compile_model(norm,n_neurons, dropout, learning_rate):\n",
    "    model = tf.keras.Sequential([\n",
    "    norm,\n",
    "    tf.keras.layers.Dense(n_neurons, activation= tf.keras.activations.relu),\n",
    "    # tf.keras.layers.Dropout(dropout),\n",
    "\n",
    "    tf.keras.layers.Dense(n_neurons, activation= tf.keras.activations.relu),\n",
    "\n",
    "    tf.keras.layers.Dense(3)\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_and_compile_model_1(shape,n_neurons, dropout, learning_rate):\n",
    "    model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(\n",
    "          16, activation='relu',\n",
    "          input_shape=shape),\n",
    "      tf.keras.layers.Dropout(0.5),\n",
    "      tf.keras.layers.Dense(3)\n",
    "     ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent of object with nan value and orginals:  0.00, 89.274189\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [41]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     53\u001B[0m model_full_name \u001B[38;5;241m=\u001B[39m  model_name \u001B[38;5;241m+\u001B[39m  \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/folds_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     55\u001B[0m early_stop, reduce_lr, tensorboard_callback, checkpoint_callback \u001B[38;5;241m=\u001B[39m   callbacks(model_full_name)\n\u001B[1;32m     57\u001B[0m history \u001B[38;5;241m=\u001B[39m dnn_model_pipe\u001B[38;5;241m.\u001B[39mfit(\n\u001B[0;32m---> 58\u001B[0m     \u001B[43mtrain_features\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m[train_index],\n\u001B[1;32m     59\u001B[0m     target\u001B[38;5;241m.\u001B[39miloc[train_index],\n\u001B[1;32m     60\u001B[0m     validation_data \u001B[38;5;241m=\u001B[39m (train_features\u001B[38;5;241m.\u001B[39miloc[test_index],target\u001B[38;5;241m.\u001B[39miloc[test_index]),\n\u001B[1;32m     61\u001B[0m     verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,\n\u001B[1;32m     62\u001B[0m    callbacks\u001B[38;5;241m=\u001B[39m[early_stop,checkpoint_callback, tensorboard_callback],\n\u001B[1;32m     63\u001B[0m     batch_size \u001B[38;5;241m=\u001B[39m batch_size\n\u001B[1;32m     64\u001B[0m )\n\u001B[1;32m     66\u001B[0m max_index_val_acc, max_value_val_acc[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28menumerate\u001B[39m(history\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_accuracy\u001B[39m\u001B[38;5;124m'\u001B[39m],), key\u001B[38;5;241m=\u001B[39moperator\u001B[38;5;241m.\u001B[39mitemgetter(\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     67\u001B[0m train_acc[i] \u001B[38;5;241m=\u001B[39m history\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m][max_index_val_acc]\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from prepared_data.completed_missing_data import remove_history_col\n",
    "import operator\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_neurons = 10\n",
    "dropout = 0.0\n",
    "learning_rate = 0.001\n",
    "batch_size = None\n",
    "\n",
    "train_h_8,_,_ = get_prepared_data(train_org, number_of_history_matches=4, map_target=False)\n",
    "\n",
    "\n",
    "\n",
    "for n_history in range(3,4):\n",
    "\n",
    "\n",
    "    # train = remove_history_col(train_h_8,number_of_history_matches=4)\n",
    "    train_features, target = split_features_target_and_map_target(train)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "    normalizer = tf.keras.layers.Input(shape=train_features.shape[1], name='input' )\n",
    "\n",
    "\n",
    "    # normalizer.adapt(np.array(train_features))\n",
    "\n",
    "    dnn_model = build_and_compile_model(normalizer,n_neurons,dropout,learning_rate)\n",
    "\n",
    "\n",
    "    model_name = f'dnn_l_{n_neurons}_l_{n_neurons}_number_of_history_matches_{4}_pipe_' + datetime.now().strftime(\"%Y:%m:%d-%H:%M:%S\")\n",
    "    output_path = 'evaluate_results/'+ model_name\n",
    "\n",
    "\n",
    "    save_results(output_path, f'lr={learning_rate} batch={batch_size}', dnn_model.summary)\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=33)\n",
    "\n",
    "    i= 0\n",
    "    max_value_val_acc = list(range(0,5))\n",
    "    min_value_val_loss = list(range(0, 5))\n",
    "    train_acc = list(range(0,5))\n",
    "    train_loss = list(range(0,5))\n",
    "\n",
    "    for train_index, test_index in skf.split(train_features,target):\n",
    "\n",
    "        model_full_name =  model_name +  f'/folds_{i}'\n",
    "\n",
    "        early_stop, reduce_lr, tensorboard_callback, checkpoint_callback =   callbacks(model_full_name)\n",
    "\n",
    "        history = dnn_model.fit(\n",
    "            train_features.iloc[train_index],\n",
    "            target.iloc[train_index],\n",
    "            validation_data = (train_features.iloc[test_index],target.iloc[test_index]),\n",
    "            verbose=0, epochs=10,\n",
    "           callbacks=[early_stop,checkpoint_callback, tensorboard_callback],\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "        max_index_val_acc, max_value_val_acc[i] = max(enumerate(history.history['val_accuracy'],), key=operator.itemgetter(1))\n",
    "        train_acc[i] = history.history['accuracy'][max_index_val_acc]\n",
    "\n",
    "        min_index_val_loss, min_value_val_loss[i] = min(enumerate(history.history['val_loss'], ), key=operator.itemgetter(1))\n",
    "        train_loss[i] = history.history['loss'][min_index_val_loss]\n",
    "\n",
    "\n",
    "        out_string = f'f_{i}\\nval_acc={max_value_val_acc[i]} train_acc={train_acc[i]} ep={max_index_val_acc+1}\\nval_loss={min_value_val_loss[i]} train_loss={train_loss[i]} ep={min_index_val_loss + 1}'\n",
    "\n",
    "        save_results(output_path, out_string)\n",
    "        i +=1\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        # dnn_model = build_and_compile_model(normalizer,n_neurons,dropout,learning_rate)\n",
    "        dnn_model = build_and_compile_model(normalizer,n_neurons,dropout,learning_rate)\n",
    "\n",
    "\n",
    "        dnn_model_pipe = make_pipeline(MinMaxScaler(),dnn_model)\n",
    "\n",
    "    out_string = f'median\\nval_acc={np.median(max_value_val_acc)} train_acc={np.median(train_acc)}\\nval_loss={np.median(min_value_val_loss)} train_loss={np.median(train_loss)}'\n",
    "\n",
    "    save_results(output_path, out_string)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}