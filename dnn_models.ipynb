{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from prepared_data.get_prepared_data import get_prepared_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from prepared_data.train_model import split_features_target_and_map_target, save_results, callbacks, columns_only_first_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrzej/PycharmProjects/Physionet_Challenge_2021/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3251: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent of object with nan value and orginals:  0.00, 92.767131\n"
     ]
    }
   ],
   "source": [
    "train_org = pd.read_csv('data/train.csv')\n",
    "train,_,_ = get_prepared_data(train_org,map_target=False, number_of_history_matches=4)\n",
    "# train = pd.read_csv('data/train_made.csv',index_col=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# train_features, target = split_features_target_and_map_target(train)\n",
    "\n",
    "# normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "# normalizer.adapt(np.array(train_features))\n",
    "\n",
    "# normalizer.trainable_variables\n",
    "# normalizer(train_features)\n",
    "# binary_col = ['is_cup','_team_history_is_play_home_1','_team_history_is_cup_1','_team_history_coach_1','_team_history_target_1_-1.0', '_team_history_target_1_0.0', '_team_history_target_1_1.0']\n",
    "\n",
    "# binary_col_all = ['is_cup']\n",
    "# binary_col_all.extend([ home_or_away +i  for i in  binary_col[1:] for home_or_away in ['home','away'] ])\n",
    "# binary_col_all\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "def build_and_compile_model(norm,n_neurons, dropout, learning_rate):\n",
    "    model = tf.keras.Sequential([\n",
    "    norm,\n",
    "    tf.keras.layers.Dense(n_neurons, activation= tf.keras.activations.relu),\n",
    "    # tf.keras.layers.Dropout(dropout),\n",
    "\n",
    "    tf.keras.layers.Dense(n_neurons, activation= tf.keras.activations.relu),\n",
    "\n",
    "    tf.keras.layers.Dense(3)\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_and_compile_model_1(shape,n_neurons, dropout, learning_rate):\n",
    "    model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(\n",
    "          n_neurons, activation='relu',\n",
    "          input_shape=shape),\n",
    "      tf.keras.layers.Dropout(dropout),\n",
    "      tf.keras.layers.Dense(3)\n",
    "     ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent of object with nan value and orginals:  0.00, 92.767131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 23:47:36.366292: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-06 23:47:36.366320: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-06 23:47:36.366337: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Zenon): /proc/driver/nvidia/version does not exist\n",
      "2022-05-06 23:47:36.366528: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: early stopping\n",
      "Epoch 00011: early stopping\n",
      "Epoch 00015: early stopping\n",
      "Epoch 00011: early stopping\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import operator\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "train,_,_ = get_prepared_data(train_org,map_target=False, number_of_history_matches=4)\n",
    "\n",
    "for dropout in range(5,6):\n",
    "\n",
    "    dropout = 0.3\n",
    "    n_neurons = 16\n",
    "    learning_rate = 0.001\n",
    "    batch_size = None\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    train_features, target = split_features_target_and_map_target(train)\n",
    "\n",
    "    dnn_model = build_and_compile_model_1( (train_features.shape[-1],),n_neurons,dropout,learning_rate)\n",
    "\n",
    "    model_name = f'final/{4}_MinMax_{n_neurons}_d_{dropout}_{n_neurons}' + datetime.now().strftime(\"%Y:%m:%d-%H:%M:%S\")\n",
    "    output_path = 'evaluate_results/'+ model_name\n",
    "\n",
    "    save_results(output_path, f'lr={learning_rate} batch={batch_size}', dnn_model.summary)\n",
    "\n",
    "    i= 0\n",
    "    max_value_val_acc = list(range(0,5))\n",
    "    min_value_val_loss = list(range(0, 5))\n",
    "    train_acc = list(range(0,5))\n",
    "    train_loss = list(range(0,5))\n",
    "\n",
    "    for train_index, test_index in StratifiedKFold(n_splits=5,shuffle=True,random_state=33).split(train_features,target):\n",
    "\n",
    "        train_features_fold  = scaler.fit_transform(train_features.iloc[train_index])\n",
    "        val_features_fold = scaler.transform(train_features.iloc[test_index])\n",
    "\n",
    "        model_full_name =  model_name +  f'/folds_{i}'\n",
    "\n",
    "        early_stop, reduce_lr, tensorboard_callback, checkpoint_callback =   callbacks(model_full_name)\n",
    "\n",
    "        history = dnn_model.fit(\n",
    "            train_features_fold,\n",
    "            target.iloc[train_index],\n",
    "            validation_data = (val_features_fold,target.iloc[test_index]),\n",
    "            verbose=0, epochs=50,\n",
    "           callbacks=[early_stop,reduce_lr,checkpoint_callback, tensorboard_callback],\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "        max_index_val_acc, max_value_val_acc[i] = max(enumerate(history.history['val_accuracy'],), key=operator.itemgetter(1))\n",
    "        train_acc[i] = history.history['accuracy'][max_index_val_acc]\n",
    "\n",
    "        min_index_val_loss, min_value_val_loss[i] = min(enumerate(history.history['val_loss'], ), key=operator.itemgetter(1))\n",
    "        train_loss[i] = history.history['loss'][min_index_val_loss]\n",
    "\n",
    "\n",
    "        out_string = f'f_{i}\\nval_acc={max_value_val_acc[i]} train_acc={train_acc[i]} ep={max_index_val_acc+1}\\nval_loss={min_value_val_loss[i]} train_loss={train_loss[i]} ep={min_index_val_loss + 1}'\n",
    "\n",
    "        save_results(output_path, out_string)\n",
    "        i +=1\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        dnn_model = build_and_compile_model_1( (train_features.shape[-1],),n_neurons,dropout,learning_rate)\n",
    "\n",
    "\n",
    "        # dnn_model = build_and_compile_model(normalizer,n_neurons,dropout,learning_rate)\n",
    "\n",
    "    out_string = f'median\\nval_acc={np.median(max_value_val_acc)} train_acc={np.median(train_acc)}\\nval_loss={np.median(min_value_val_loss)} train_loss={np.median(train_loss)}'\n",
    "\n",
    "    save_results(output_path, out_string)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}